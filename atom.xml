<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LATTE world</title>
  <subtitle>Semiconductor and Data.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://kzhoulatte.github.io/"/>
  <updated>2018-03-28T18:05:29.000Z</updated>
  <id>https://kzhoulatte.github.io/</id>
  
  <author>
    <name>Kuan Zhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>reflect</title>
    <link href="https://kzhoulatte.github.io/2018/03/28/reflect/"/>
    <id>https://kzhoulatte.github.io/2018/03/28/reflect/</id>
    <published>2018-03-28T10:57:33.000Z</published>
    <updated>2018-03-28T18:05:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>现在每次看到po出的生活感想，不管是矫情的还是一本正经的，若不是观点新奇头头是道，我就是内心里的想笑，带有非常正面的理解。</p>
<p>生活本来就是一锅粥，你越搅越乱。</p>
<p>因为这是一个庞大的多体问题，而且还有数不尽的隐藏变量，我们是找不到最优解的。能做的不过是找到自己在意的那几个目标，然后假装自己把其他的忘记了。</p>
<p>几十年，把自己在意的事情做到了，已经是很了不起的事情了，已经足够我吹嘘了。</p>
<p>而这些在意的事其实都有他的诀窍，摸索这些就已经是我的心血所在。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在每次看到po出的生活感想，不管是矫情的还是一本正经的，若不是观点新奇头头是道，我就是内心里的想笑，带有非常正面的理解。&lt;/p&gt;
&lt;p&gt;生活本来就是一锅粥，你越搅越乱。&lt;/p&gt;
&lt;p&gt;因为这是一个庞大的多体问题，而且还有数不尽的隐藏变量，我们是找不到最优解的。能做的不过是
    
    </summary>
    
    
      <category term="thoughts" scheme="https://kzhoulatte.github.io/tags/thoughts/"/>
    
  </entry>
  
  <entry>
    <title>english</title>
    <link href="https://kzhoulatte.github.io/2018/03/26/english/"/>
    <id>https://kzhoulatte.github.io/2018/03/26/english/</id>
    <published>2018-03-26T09:13:29.000Z</published>
    <updated>2018-03-26T16:40:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>There is a very important lesson I just learned about English. </p>
<p>Keep talking or try talking in the professional or authentic way is not for some specific time use, instead helping generate the vacabulary which will help a lot when writing. Thus you could stop thinking of quoting damn old sentences when writing new important materials. Honestly I like being the person who do better than talking more, but I need write in a cool way.  </p>
<p>Things will accumulate and snowball.   </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;There is a very important lesson I just learned about English. &lt;/p&gt;
&lt;p&gt;Keep talking or try talking in the professional or authentic way i
    
    </summary>
    
    
      <category term="english" scheme="https://kzhoulatte.github.io/tags/english/"/>
    
  </entry>
  
  <entry>
    <title>myself</title>
    <link href="https://kzhoulatte.github.io/2018/03/09/myself/"/>
    <id>https://kzhoulatte.github.io/2018/03/09/myself/</id>
    <published>2018-03-09T20:08:27.000Z</published>
    <updated>2018-03-19T23:53:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>写点关于自己的事。<br>最近因为参加APS和不断的投简历和收面试，在想一些以前的事。其中的起因是关注了magic angle superconductor的事和收到了几家大厂的interview面试邀请，如Jane Street/SIG LLP/Google。</p>
<p>七八岁的时候虽然还很喜欢赖在爸妈边上，但是慢慢开始喜欢摸麻将，看得懂麻将，而且能够盲摸麻将；一年级二年级两年除了一次语文97，其余都是100；四年级开始参加数学竞赛，而且市里比赛，91分是我们乡学校第一次拿的一等奖；有一段时间特别喜欢和伯下军旗，慢慢进步；经常带数学竞赛题目回家，然后有时候被爸爸做了，会不高兴，所以会争着做；开始喜欢象棋和算24点，24点很少输，象棋不是很擅长；初中入学考试是乡中学的第一，不过在市中学不算好；市中学第一次期中考试全班第一；在班级里一段时间代替老师中午答疑；同时参加所有学科竞赛，虽然没有出彩成绩但是数学，科学，计算机都有些奖项；初中英语很出彩，经常被英语老师表扬；初中是班主任最看好的一个，因为觉得不偏科，而且成绩好；初中基本没有额外用功的概念，每天放学只会去打球，甚至跟着同学去网吧和台球；初中最终市前100，升入高中拿到奖金1万；高中第一次期中考，全年级段第二；高一参加全高中数学竞赛，因为策略比较好，拿到了绍兴市二等奖，比较轰动；高二开始是唯二之一的参加两个竞赛，也开始慢慢有动力；因为受伤和一些其他关系，成绩有所下降，但是高考拿到了市第五；因为高二一口气看完的爱因斯坦传选择了物理，成绩在科大一直不上不下，但是最好的一个学期拿到了4.03/4.3；最难的数理方程以及其他数学课都拿到了90以上；六个月之内参加kaggle比赛，最终到了天梯1%；</p>
<p>其他的比如，篮球／台球／24点／扑克／麻将／狼人杀／滑雪／羽毛球／基本都达到了above average的大概八九十水平；我爸确实是我见过的人里算很聪明的了；</p>
<p>随想而已，祝自己好运，然后就这么断了吧。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;写点关于自己的事。&lt;br&gt;最近因为参加APS和不断的投简历和收面试，在想一些以前的事。其中的起因是关注了magic angle superconductor的事和收到了几家大厂的interview面试邀请，如Jane Street/SIG LLP/Google。&lt;/p&gt;
&lt;
    
    </summary>
    
    
      <category term="myself" scheme="https://kzhoulatte.github.io/tags/myself/"/>
    
  </entry>
  
  <entry>
    <title>grandpa</title>
    <link href="https://kzhoulatte.github.io/2018/02/28/grandpa/"/>
    <id>https://kzhoulatte.github.io/2018/02/28/grandpa/</id>
    <published>2018-02-28T13:27:50.000Z</published>
    <updated>2018-02-28T21:32:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>外公, Pray for you. 希望您从始至终都是开开心心的没有遗憾, 我们都会想念您的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;外公, Pray for you. 希望您从始至终都是开开心心的没有遗憾, 我们都会想念您的。&lt;/p&gt;

    
    </summary>
    
    
      <category term="grandpa" scheme="https://kzhoulatte.github.io/tags/grandpa/"/>
    
  </entry>
  
  <entry>
    <title>TopK</title>
    <link href="https://kzhoulatte.github.io/2018/02/07/TopK/"/>
    <id>https://kzhoulatte.github.io/2018/02/07/TopK/</id>
    <published>2018-02-07T18:33:33.000Z</published>
    <updated>2018-02-08T02:53:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>重新过了一遍Hadoop projects的内容，其中主要分析基于N Gram模型和word count的google search autocomplete/基于mapreduce乘法的Pagerank／TopK／协同过滤的Recommender System。</p>
<p>这里想把里面的主要思想简单的记录一下。<br>首先google search autocomplete的想法挺简单的，主要就是N Gram模型下的word count；<br>Pagerank和Recommender System的算法最后是类似的，主要是基于mapreduce情况下的矩阵乘法；从算法思想里来说，TopK反而是最tricky的，也是最容易考的面试内容。</p>
<p>其中TopK首先需要hash table进行map分类，进行不同的word count；然后通过heapq进行topk的算法，理想结果是nlog(k)复杂度；同时需要考虑一个细节是：如果某个item freq太高使得某个node负担太大，则需要额外加入下标_01,_02进行sample处理；最后某个node需要负责把不同下标的相同item结果整合，最后再把局部TopK集合计算整体的TopK。</p>
<p>这其中也可以用到database。</p>
<p>其实涉及了不少Big Data的思路。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;重新过了一遍Hadoop projects的内容，其中主要分析基于N Gram模型和word count的google search autocomplete/基于mapreduce乘法的Pagerank／TopK／协同过滤的Recommender System。&lt;/p&gt;

    
    </summary>
    
    
      <category term="bigdata" scheme="https://kzhoulatte.github.io/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>instacart</title>
    <link href="https://kzhoulatte.github.io/2018/01/14/instacart/"/>
    <id>https://kzhoulatte.github.io/2018/01/14/instacart/</id>
    <published>2018-01-15T00:03:36.000Z</published>
    <updated>2018-01-15T10:46:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>Instacart算是我第一个kaggle比赛，幸运拿到了85th of 2623的名次，其实这个比赛的内容还是很丰富的，比如类似数据库结构的datasets、包含简单的word2vec、feature engineering、F1 maximization、CV、PCA、调参以及ensemble。</p>
<p>虽然没有很fancy的模型和技巧，但是feature engineering和对数据的理解还是挺重要的。<br>整理的pdf presentation file：</p>
<p><a href="https://drive.google.com/open?id=13KDcnEjAuhHGU22bs1ipVQtJihS2Lw-Q" target="_blank" rel="external">https://drive.google.com/open?id=13KDcnEjAuhHGU22bs1ipVQtJihS2Lw-Q</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Instacart算是我第一个kaggle比赛，幸运拿到了85th of 2623的名次，其实这个比赛的内容还是很丰富的，比如类似数据库结构的datasets、包含简单的word2vec、feature engineering、F1 maximization、CV、PCA、
    
    </summary>
    
    
      <category term="kaggle" scheme="https://kzhoulatte.github.io/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>santamatch</title>
    <link href="https://kzhoulatte.github.io/2018/01/14/santamatch/"/>
    <id>https://kzhoulatte.github.io/2018/01/14/santamatch/</id>
    <published>2018-01-14T23:47:09.000Z</published>
    <updated>2018-01-15T08:00:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>关于projects, 虽然在santa match challenge里成功使用了ortools里的min cost max flow method，对于问题的formation是基本了解的，但是对于这个图论算法本身不了解，比如为什么这个图论算法可以这么快解决本是NP hard的integer programming问题。</p>
<p>稍微了解之后，发现mincostmaxflow在Computer vision还有很多应用。<br>而对于这个图论问题的解法本身，这里简单记录：</p>
<p>Min Cost Max Flow的问题主要解决思路是：最大流的条件是当且仅当residual network不存在Augmenting Path。而Augmenting path定义：假如有这么一条路，这条路从源点开始一直一段一段的连到了汇点，并且，这条路上的每一段都满足流量 &lt; 容量。那么，我们一定能找到这条路上的每一段的(容量-流量)的值当中的最小值delta。我们把这条路上每一段的流量都加上这个delta，一定可以保证这个流依然是可行流。这样我们就得到了一个更大的流，他的流量是之前的流量+delta，而这条路就叫做增广路径。</p>
<p>再看之后的，不得不说不是时候，暂且放下其余的细节。 </p>
<p>贴个索引：<a href="http://mindlee.com/2011/11/19/network-flow/" target="_blank" rel="external">http://mindlee.com/2011/11/19/network-flow/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于projects, 虽然在santa match challenge里成功使用了ortools里的min cost max flow method，对于问题的formation是基本了解的，但是对于这个图论算法本身不了解，比如为什么这个图论算法可以这么快解决本是NP h
    
    </summary>
    
    
      <category term="kaggle" scheme="https://kzhoulatte.github.io/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>apply</title>
    <link href="https://kzhoulatte.github.io/2018/01/14/apply/"/>
    <id>https://kzhoulatte.github.io/2018/01/14/apply/</id>
    <published>2018-01-14T19:02:54.000Z</published>
    <updated>2018-01-15T03:38:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>六点多听Data App Lab的讲座，Jason老师提到明天是节假日，我才想起来明天休息啊，这也是小惊喜吧。整理了一下我之前的文件夹等之后，加上下午以及周五晚上和师兄聊了好久之后，觉得可以稍微写点现在的想法。</p>
<p>首先kaggle算是有了不错的成绩，接近全服top 1%了，剩下的gold medal以及master只能说走着瞧了，剩下的精力应该主要都放在面试准备上了。</p>
<p>同样的data scientist其实不同的工作内容性质也相差很大，待遇也是。硅谷偏R&amp;D以及algorithms的需要比较强的CS和数理基础，level4大概能到25~30的package，而一般的analytics相关的，比较偏向于数据分析处理，哪怕flag公司，比较好的也是15~20，而一般的平均水平大概在10~15。所以其实选择的余地和弹性还是很大。</p>
<p>kaggle目前来说，虽然评价不一，但无疑是转专业的利器，但是部分小公司还是很认可的。</p>
<p>这里先以R&amp;D的要求看自己吧，kaggle虽然做了不少，但是动手能力还是不算强，统计和算法能力不够。</p>
<p>现在的想法是：kaggle暂时放下了，更多时间把简历相关的细节搞清楚(know your resume)，把kaggle相关的经验消化清楚，推一推公式，把自己的理解整理出来，包括通过github把流程规范化，同时提高from scratch的能力或者说不google的情况下的能力。提高统计／分析／算法／优化相关的背景，把自己的理解表达规范化，英语交流能力也可以提高一下。</p>
<p>当然以后把这些理解写在这里。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;六点多听Data App Lab的讲座，Jason老师提到明天是节假日，我才想起来明天休息啊，这也是小惊喜吧。整理了一下我之前的文件夹等之后，加上下午以及周五晚上和师兄聊了好久之后，觉得可以稍微写点现在的想法。&lt;/p&gt;
&lt;p&gt;首先kaggle算是有了不错的成绩，接近全服to
    
    </summary>
    
    
      <category term="jobhunting" scheme="https://kzhoulatte.github.io/tags/jobhunting/"/>
    
  </entry>
  
  <entry>
    <title>competition</title>
    <link href="https://kzhoulatte.github.io/2017/12/08/competition/"/>
    <id>https://kzhoulatte.github.io/2017/12/08/competition/</id>
    <published>2017-12-09T02:53:40.000Z</published>
    <updated>2017-12-09T11:07:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天学习了一下kaggle competition的challenge和coursera的how to win kaggle，其实收获还是很多的，但是细节还得慢慢消化。这里在睡前稍微整理一下：</p>
<p>重点：1）Ploting for EDA; 2) Statistical inference: t-test,chi squared test, etc; 3)Model分类和划分特点；4）根据不同model的feature scaling，feature transformation，outlier，fillna等；5）一些特别的feature的产生和处理方式，比如images／texts；6）Data leaks也是重要的；7）Bag of words和word2vec相关；8）不同metrics的分析以及特点，利用；9）利用PCA等产生特别的feature；等等。</p>
<p>重点的重点还是通过EDA对数据的特点有很好的认识，并和model和metrics结合起来。</p>
<p>包括AUC等。</p>
<p>No free lunch theorm，所以多思考多尝试吧。</p>
<p>ps：稍微有点了NLP的兴趣，但是还是先刷题吧，就酱。 </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天学习了一下kaggle competition的challenge和coursera的how to win kaggle，其实收获还是很多的，但是细节还得慢慢消化。这里在睡前稍微整理一下：&lt;/p&gt;
&lt;p&gt;重点：1）Ploting for EDA; 2) Statisti
    
    </summary>
    
    
      <category term="kaggle" scheme="https://kzhoulatte.github.io/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>Black-Scholes</title>
    <link href="https://kzhoulatte.github.io/2017/12/02/Black-Scholes/"/>
    <id>https://kzhoulatte.github.io/2017/12/02/Black-Scholes/</id>
    <published>2017-12-02T14:42:18.000Z</published>
    <updated>2017-12-03T00:10:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天早上听了一节关于Quant Interview的公开课，对比之前相同老师不同内容的另外一次，觉得确实还是明白了很多，对这个领域自我觉得算是入门了，理解还不够。</p>
<p>然后看了两章的Pricing the future, 眼前有十七八世纪那种简易交易所外面熙熙攘攘的感觉，同时确实觉得在这样的环境下，法国人雷格纳特在交易所做事，能够慢慢领悟出一本书的规律-《概率计算和股票交易哲学》同时还归纳出数学表示，甚至用虚数试图的解释。这应该就是所谓的talent吧。</p>
<p>同时下午打算好好研究一下量化定价的核心（圣杯）-BS公式。stochastic calculus/arbitrage/risk neutral这些都是关键的，暂放了。</p>
<p>根据之前看edx课程以及下午的研究，先暂时写下自己对于fixed income金融衍生品的理解：</p>
<p>金融衍生品的出现其实挺正常的，因为人们想要规避风险，但是后来就需要考虑规避风险是要付出代价的，所以需要考虑多少代价合适。所以最早也是最基本的思路，在没有任何模型的基础上，是可以通过replicating portfolio计算的，把一个portfolio的payoff当作期权的价格。这里用到的基本想法是市场上有那么多talent那么多眼睛，市场不应该存在arbitrage的机会，即无风险套利。</p>
<p>但是这些只适用于简单的options，对于复杂的计算，还是需要一些模型处理。</p>
<p>这时最先开始的应该是风险中立定价方法和鞅定价方法。这两种是从模型上给定了假设基础。比如风险中立定价虽然，严格来说不是假设，但是却提供了一个最简单明白的测度空间。他把期权的价格和投资者的风险偏好直接去除了，告诉我们衡量期权价格时，应该只关注于期权本身的期望以及其无风险折现的过程。但其实模型对于非风险中立测度空间也是成立的。这些可以参考：<a href="https://www.zhihu.com/question/26724322。" target="_blank" rel="external">https://www.zhihu.com/question/26724322。</a></p>
<p>接着另外的是鞅定价模型，鞅首先就是假设赌博或者说足够随机的股票过程中，这一次的期望是和上一次的赌博无关的。所以有时候，求价格时，可以计算股价随机过程，使得在风险中立测度下更简单计算。</p>
<p>从而来到了股票价格的随机过程描述。首先这里再想到雷格纳特自己观察到了股价变化和时间根的关系，也该算是这一切的开始吧。股价最重要的一个假设应该是对数正态分布，而这又是时间序列的一个共有的特性，都可以从复利的角度理解吧。</p>
<p>然后通过这些和布朗运动的联系，又有了主要的两个定价方法，二项定价模型和BSM模型：</p>
<p>二项定价模型，其实比较简单，但是又是非常强大。他只把股票每个时间点的变化想象的只有两种方式，但是如果时间足够密，又能够包含了庞大的可能性；</p>
<p>BSM模型，七大假设就先不罗列了。推导思路就是根据对数正态分布的性质，分别推导期权是否被执行的P和执行所得E的表示，最后把正太分布的性质等都参杂在一起，也就变成了包含d1，d2的一个看起来很吓人的公式。</p>
<p>这样到此为止，最大的问题就是布朗运动，维纳过程是如何很好的解释清楚这些模型的基础的。</p>
<p>维纳工程和布朗运动类似，而在伊藤引理的框架下，股价或者其他金融产品是服从，一个固定利率加上一个随机过程的微风方程的，而在这个方程和stochastic calculus的框架下，其实就可以推导出BS formula等结论。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天早上听了一节关于Quant Interview的公开课，对比之前相同老师不同内容的另外一次，觉得确实还是明白了很多，对这个领域自我觉得算是入门了，理解还不够。&lt;/p&gt;
&lt;p&gt;然后看了两章的Pricing the future, 眼前有十七八世纪那种简易交易所外面熙熙攘攘
    
    </summary>
    
    
      <category term="quant" scheme="https://kzhoulatte.github.io/tags/quant/"/>
    
  </entry>
  
  <entry>
    <title>最近所看-犹太恩怨</title>
    <link href="https://kzhoulatte.github.io/2017/12/01/%E6%9C%80%E8%BF%91%E6%89%80%E7%9C%8B/"/>
    <id>https://kzhoulatte.github.io/2017/12/01/最近所看/</id>
    <published>2017-12-01T13:23:01.000Z</published>
    <updated>2017-12-01T21:46:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>在看Pricing the future的时候，讲到了首位书面阐述期权的人-约瑟夫.德拉维加，是1650年出生于马拉诺家庭的一位犹太人，而这个名词马拉诺家庭则是指被西班牙宗教裁判力量而强制改信基督教的犹太人。这里先不说德拉维加的书是多大影响，恩恩怨怨搞出这么多的专有名词。我最好奇的是，犹太人和基督教到底是因为什么结怨，成了这么多事情的导火索。</p>
<p>学习了以后，发现主要原因是耶稣的身份问题，其他还有很多：<br>1）犹太教是西亚的希伯来人发展起来的宗教，其实基督教是在犹太教的基础上发展起来的；<br>2）犹太教的主要教义是圣经.旧约，而不太承认新约;<br>3）除了一些其他教义外，犹太教尤其不承认耶稣就是预言中的神的儿子也就是所谓的弥赛亚，而相信真正的弥赛亚还没来到；<br>4）犹太教不相信耶稣的原因有很多，包括很多没有实现的预言，基督教说你们也太没耐心了，耶稣会”再次降临的，等他再荣耀归来，会审判一切死人活人，信从他的将得永生”；但其实可能最主要的原因就是耶稣既不是犹太人也不帮犹太人说话，不带领犹太人回以色列；<br>5）耶稣的死刑是被犹太祭司所害；<br>6）美国还是很支持以色列以及耶路撒冷的；</p>
<p>关于锡安主义，我也精神上支持，不知道我属不属于锡安主义了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在看Pricing the future的时候，讲到了首位书面阐述期权的人-约瑟夫.德拉维加，是1650年出生于马拉诺家庭的一位犹太人，而这个名词马拉诺家庭则是指被西班牙宗教裁判力量而强制改信基督教的犹太人。这里先不说德拉维加的书是多大影响，恩恩怨怨搞出这么多的专有名词。我
    
    </summary>
    
    
      <category term="reading" scheme="https://kzhoulatte.github.io/tags/reading/"/>
    
  </entry>
  
  <entry>
    <title>leetcode</title>
    <link href="https://kzhoulatte.github.io/2017/10/02/leetcode/"/>
    <id>https://kzhoulatte.github.io/2017/10/02/leetcode/</id>
    <published>2017-10-02T16:27:23.000Z</published>
    <updated>2017-10-02T23:32:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>Leetcode 博客</p>
<p>“刷题是开心的，游戏是苦逼的”<br>刷题博客，记录刷题过程，总结算法，效率优先。<br>同时进行Big Data的Projects训练。</p>
<p>Edx pricing options，系统设计, Research, Kaggle等。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Leetcode 博客&lt;/p&gt;
&lt;p&gt;“刷题是开心的，游戏是苦逼的”&lt;br&gt;刷题博客，记录刷题过程，总结算法，效率优先。&lt;br&gt;同时进行Big Data的Projects训练。&lt;/p&gt;
&lt;p&gt;Edx pricing options，系统设计, Research, Kaggl
    
    </summary>
    
    
      <category term="leetcode" scheme="https://kzhoulatte.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop and Spark</title>
    <link href="https://kzhoulatte.github.io/2017/06/01/HadoopSpark/"/>
    <id>https://kzhoulatte.github.io/2017/06/01/HadoopSpark/</id>
    <published>2017-06-01T18:07:57.000Z</published>
    <updated>2017-06-02T04:24:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习方向其实确实太热了，Hadoop和Spark的需求可能比机器学习方向更合适。</p>
<p>之后学习补充一下Hadoop和Spark的入门。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习方向其实确实太热了，Hadoop和Spark的需求可能比机器学习方向更合适。&lt;/p&gt;
&lt;p&gt;之后学习补充一下Hadoop和Spark的入门。&lt;/p&gt;

    
    </summary>
    
    
      <category term="bigdata" scheme="https://kzhoulatte.github.io/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>Job hunting</title>
    <link href="https://kzhoulatte.github.io/2017/05/31/Jobhunting/"/>
    <id>https://kzhoulatte.github.io/2017/05/31/Jobhunting/</id>
    <published>2017-05-31T13:49:16.000Z</published>
    <updated>2017-05-31T20:52:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>和刚找到Facebook Data Engineer工作的师兄聊天，其实发现最后可能是因为Hadoop等背景，Kaggle如果已经有不错的成绩，Hadoop这些才是加分项。</p>
<p>先Mark一下。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;和刚找到Facebook Data Engineer工作的师兄聊天，其实发现最后可能是因为Hadoop等背景，Kaggle如果已经有不错的成绩，Hadoop这些才是加分项。&lt;/p&gt;
&lt;p&gt;先Mark一下。&lt;/p&gt;

    
    </summary>
    
    
      <category term="jobhunting" scheme="https://kzhoulatte.github.io/tags/jobhunting/"/>
    
  </entry>
  
  <entry>
    <title>GeoGuessr</title>
    <link href="https://kzhoulatte.github.io/2017/05/28/GeoGuessr/"/>
    <id>https://kzhoulatte.github.io/2017/05/28/GeoGuessr/</id>
    <published>2017-05-28T13:40:10.000Z</published>
    <updated>2017-05-28T20:41:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>试玩了一下GeoGuessr.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;试玩了一下GeoGuessr.&lt;/p&gt;

    
    </summary>
    
    
      <category term="fun" scheme="https://kzhoulatte.github.io/tags/fun/"/>
    
  </entry>
  
  <entry>
    <title>GPU Architecture</title>
    <link href="https://kzhoulatte.github.io/2017/05/10/GPUArchitecture/"/>
    <id>https://kzhoulatte.github.io/2017/05/10/GPUArchitecture/</id>
    <published>2017-05-10T18:23:02.000Z</published>
    <updated>2017-05-12T00:14:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近NVIDIA的股票又在疯涨。<b><br>AMD的Vega在GDC大会上公布，NVIDIA的Volta架构GV100也在GTC2017上曝光。</b></p>
<p>继Tesla/Fermi/Kepler/Maxwell/Pascal之后，Volta的新架构备受瞩目。<b><br>挖坑学习一下GPU架构及原理。</b></p>
<p>参考：<a href="https://chenrudan.github.io/blog/2015/12/20/introductionofgpuhardware.html" target="_blank" rel="external">https://chenrudan.github.io/blog/2015/12/20/introductionofgpuhardware.html</a></p>
<p>里面已经写得非常清楚详细，主要的总结是：</p>
<p>（1）显卡结构本质上没什么区别，注意核心参数：处理器核心，工作频率，显存位宽，单卡or双卡。对于深度学习，因为数据量大，核心数和显存大小比较重要。同时需要注意CUDA需要让所有的core都工作。</p>
<p>（2）显卡的SM流处理器簇是GPU的核心部分，其中包括了各种调度器共享内存等等。同时不同架构的区别往往在于SM的设计，比如shared memory大小，比如FP unit，INT unit，DP unit的相互比例关系。这些关系都可以决定这块显卡的性能以及适合做什么工作。（ps：深度学习使用的M系列显卡是单精度的，深度学习并不要求双精度）</p>
<p>（3）TESLA的K型号卡适合高性能科学计算，同时支持双精度，但是不怎么适合深度学习。</p>
<p>（4）针对不同的应用，开源库都已经挺完整。卷积计算：cudnn；卷积神经网络：caffe，torch；rnn：mxnet，tensorflow等。</p>
<p>（5）相互的架构连接以及内存技术也一直在更新，比如ECC内存，HBM内存。</p>
<p>（6）如果使用多张显卡，注意电源的选择。</p>
<p>关于AMD和NVIDIA的显卡区别，目前看主要的有：</p>
<p>（1）两者面向更高计算性能的途径不一样，NVIDIA倾向于more capable stream processors allowing for slightly more complex calculations; 而AMD倾向smaller, less complex stream processors。所以两者在不同类型的计算任务上性能各有优劣。</p>
<p>（2）在图像显示游戏性能方面，虽然也各有优劣。但是由于目前只有CUDA完美支持Physx物理加速（Physx可以在GPU或加速卡），所以N卡比较占优。</p>
<p>而新的GV100芯片，除了先进的L1 Cache和Memory等技术加线程调度技术，主要新特性是加入了Tensor Core，所以非常适合深度学习。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近NVIDIA的股票又在疯涨。&lt;b&gt;&lt;br&gt;AMD的Vega在GDC大会上公布，NVIDIA的Volta架构GV100也在GTC2017上曝光。&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;继Tesla/Fermi/Kepler/Maxwell/Pascal之后，Volta的新架构备受瞩目。&lt;
    
    </summary>
    
    
      <category term="GPU" scheme="https://kzhoulatte.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>Keras含义</title>
    <link href="https://kzhoulatte.github.io/2017/05/03/Keras/"/>
    <id>https://kzhoulatte.github.io/2017/05/03/Keras/</id>
    <published>2017-05-03T16:34:07.000Z</published>
    <updated>2017-05-03T23:46:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>虽然Keras已经不被很多人推荐，但是觉得它的含义挺有趣。</p>
<p>Keras 是基于Theano的一个深度学习框架，它的设计参考了Torch，用Python编写，是一个高度模块化的神经网络库，支持GPU和CPU。</p>
<p>Keras 是古希腊语中”角”的意思, 最早出现在古希腊的《奥德赛》. 梦神被分为两派. 一派用虚假景象欺骗人们,他们通过象牙之门来到地面. 一派启示即将经历的未来, 他们通过角之门来到地面. 所以Keras代表“真实/实现”的一方，与“虚幻/欺骗”相对。就如它的名字，Keras 吹响向“深度学习”前进的号角，扫除“深度学习”高不可攀的虚相。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然Keras已经不被很多人推荐，但是觉得它的含义挺有趣。&lt;/p&gt;
&lt;p&gt;Keras 是基于Theano的一个深度学习框架，它的设计参考了Torch，用Python编写，是一个高度模块化的神经网络库，支持GPU和CPU。&lt;/p&gt;
&lt;p&gt;Keras 是古希腊语中”角”的意思,
    
    </summary>
    
    
      <category term="deep learning" scheme="https://kzhoulatte.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>RPA understanding</title>
    <link href="https://kzhoulatte.github.io/2017/04/20/RPA-understanding/"/>
    <id>https://kzhoulatte.github.io/2017/04/20/RPA-understanding/</id>
    <published>2017-04-20T16:34:02.000Z</published>
    <updated>2017-04-25T06:22:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>RPA here is Random Phase Approximation, which is one of the basis approximation in DFT calculation and condensed matter physics. This is a note which I gradually got better understanding of it. <b></b></p>
<p>In the RPA, electrons are assumed to respond only to the total electric potential V(r) which is the sum of the external perturbing potential Vext(r) and a screening potential Vsc(r). The external perturbing potential is assumed to oscillate at a single frequency ω, so that the model yields via a self-consistent field (SCF) method a dynamic dielectric function denoted by εRPA(k, ω).<b></b></p>
<p>The contribution to the dielectric function from the total electric potential is assumed to average out, so that only the potential at wave vector k contributes. This is what is meant by the random phase approximation. <b></b></p>
<p>In DFT, the treatment of exchange and correlation in terms of ‘‘exact-exchange plus correlation in the random-phase approximation’’ offers a promising avenue in accurate simulations.</p>
<p>From wiki.com and article:”Random-phase approximation and its applications in computational chemistry and materials science”, DOI 10.1007/s10853-012-6570-4.  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RPA here is Random Phase Approximation, which is one of the basis approximation in DFT calculation and condensed matter physics. This is 
    
    </summary>
    
    
      <category term="notes" scheme="https://kzhoulatte.github.io/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>2D materials-My understanding</title>
    <link href="https://kzhoulatte.github.io/2017/04/17/2Dmaterials/"/>
    <id>https://kzhoulatte.github.io/2017/04/17/2Dmaterials/</id>
    <published>2017-04-17T14:00:19.000Z</published>
    <updated>2017-04-19T01:36:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>我的PhD研究领域是2D materials的光学和电学性质，尤其是电子输运性质。<b><br>一些”科普向”的理解：<b></b></b></p>
<p>摘自<a href="https://www.scientificamerican.com/article/graphene-finally-gets-an-electronic-on-off-switch/" target="_blank" rel="external">https://www.scientificamerican.com/article/graphene-finally-gets-an-electronic-on-off-switch/</a>:<b><br>“Semiconductors are defined by their band gap: the energy required to excite an electron stuck in the valence band, where it cannot conduct electricity, to the conduction band, where it can. The band gap needs to be large enough so that there is a clear contrast between a transistor’s on and off states, and so that it can process information without generating errors.” <b></b></b></p>
<p>Relaxation time and charge density as a function of energy level calculation for graphene: (literature reading…)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我的PhD研究领域是2D materials的光学和电学性质，尤其是电子输运性质。&lt;b&gt;&lt;br&gt;一些”科普向”的理解：&lt;b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;摘自&lt;a href=&quot;https://www.scientificamerican.com/article/graph
    
    </summary>
    
    
      <category term="2Dmaterials" scheme="https://kzhoulatte.github.io/tags/2Dmaterials/"/>
    
  </entry>
  
  <entry>
    <title>coding</title>
    <link href="https://kzhoulatte.github.io/2017/04/12/coding/"/>
    <id>https://kzhoulatte.github.io/2017/04/12/coding/</id>
    <published>2017-04-13T02:28:50.000Z</published>
    <updated>2017-04-13T09:59:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>&ensp; &ensp; 好好研究了一下Matlab的3D画图功能，参考书籍是《精通Matlab》张志涌。才发现在plot3，meshgrid和surf三个基本命令之后，Matlab的潜能才刚刚是个开始。<b><br>&ensp; &ensp; colormap加上shading，view，light，material，alpha几个参数命令，由colormap自然可以得到很多绚丽配色的图，惊喜是light和material的调整就好像摄影里的灯光和美工一样，能够立马让图像获得质感，可以制作符合出版要求的图像。<b><br>&ensp; &ensp; 同时，一个小发现是，github以及matlab自己的file exchange上，有很多人开发了各种matlab schemer，可以修改matlab的界面。尝试了之后觉得还不错，比较喜欢sublime的仿制版本。<b><br>&ensp; &ensp; 说到sublime这个被称作性感编辑器的家伙, 下载了之后打开确实小有惊艳，非常有质感的编辑器，加上那么高的灵活性和传说中的各种神插件。我就把它先默默放在那里吧，以后需要的时候，再好好研究一下。<b></b></b></b></b></p>
<p>&ensp; &ensp; Coding… </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;ensp; &amp;ensp; 好好研究了一下Matlab的3D画图功能，参考书籍是《精通Matlab》张志涌。才发现在plot3，meshgrid和surf三个基本命令之后，Matlab的潜能才刚刚是个开始。&lt;b&gt;&lt;br&gt;&amp;ensp; &amp;ensp; colormap加上sha
    
    </summary>
    
    
      <category term="coding" scheme="https://kzhoulatte.github.io/tags/coding/"/>
    
  </entry>
  
</feed>
